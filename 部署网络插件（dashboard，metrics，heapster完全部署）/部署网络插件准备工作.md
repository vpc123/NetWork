## 部署网络插件（dashboard，metrics，heapster完全部署）

在master节点上允许hpa通过接口采集数据

    $vi /etc/kubernetes/manifests/kube-controller-manager.yaml
    - --horizontal-pod-autoscaler-use-rest-clients=false

master上允许istio的自动注入，修改/etc/kubernetes/manifests/kube-apiserver.yaml

    $vi /etc/kubernetes/manifests/kube-apiserver.yaml
    - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota

所有master节点重启服务

    $systemctl restart kubelet

下载dashboard镜像

    $docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.8.3
    $docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.8.3 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.3

下载prometheus和traefik

    # prometheus
    $docker pull prom/prometheus:v2.3.1
    
    # traefik
    $docker pull traefik:v1.6.3

下载istio，istio是一个强大的服务网格，可用于灰度发布等强大功能，[文档点我](https://preliminary.istio.io/zh/docs/concepts/what-is-istio/ "走你")，不需要可无需下载


    # istio
    docker pull docker.io/jaegertracing/all-in-one:1.5
    docker pull docker.io/prom/prometheus:v2.3.1
    docker pull docker.io/prom/statsd-exporter:v0.6.0
    docker pull quay.io/coreos/hyperkube:v1.7.6_coreos.0
    docker pull istio/citadel:1.0.0
    docker tag istio/citadel:1.0.0  gcr.io/istio-release/citadel:1.0.0
    
    docker pull istio/galley:1.0.0
    docker tag istio/galley:1.0.0  gcr.io/istio-release/galley:1.0.0
    
    docker pull istio/grafana:1.0.0
    docker tag istio/grafana:1.0.0  gcr.io/istio-release/grafana:1.0.0
    
    docker pull istio/mixer:1.0.0
    docker tag istio/mixer:1.0.0  gcr.io/istio-release/mixer:1.0.0
    docker pull istio/pilot:1.0.0
    docker tag istio/pilot:1.0.0 gcr.io/istio-release/pilot:1.0.0
    
    docker pull istio/proxy_init:1.0.0
    docker tag istio/proxy_init:1.0.0  gcr.io/istio-release/proxy_init:1.0.0
    docker pull istio/proxyv2:1.0.0
    docker tag istio/proxyv2:1.0.0  gcr.io/istio-release/proxyv2:1.0.0
    docker pull istio/servicegraph:1.0.0
    docker tag istio/servicegraph:1.0.0  gcr.io/istio-release/servicegraph:1.0.0
    docker pull istio/sidecar_injector:1.0.0
    docker tag istio/sidecar_injector:1.0.0  gcr.io/istio-release/sidecar_injector:1.0.0

下载metric和heapster

    docker pull dotbalo/heapster-amd64:v1.5.4
    docker pull dotbalo/heapster-grafana-amd64:v5.0.4
    docker pull dotbalo/heapster-influxdb-amd64:v1.5.2
    docker pull dotbalo/metrics-server-amd64:v0.2.1
    
    
    docker tag dotbalo/heapster-amd64:v1.5.4k8s.gcr.io/heapster-amd64:v1.5.4
    docker tag dotbalo/heapster-grafana-amd64:v5.0.4k8s.gcr.io/heapster-influxdb-amd64:v1.5.2
    docker tag dotbalo/heapster-influxdb-amd64:v1.5.2   k8s.gcr.io/heapster-grafana-amd64:v5.0.4
    docker tag dotbalo/metrics-server-amd64:v0.2.1  gcr.io/google_containers/metrics-server-amd64:v0.2.1

Master节点允许master部署pod

    $kubectl taint nodes --all node-role.kubernetes.io/master-

master节点上安装metrics-server，从v1.11.0开始，性能采集不再采用heapster采集pod性能数据，而是使用metrics-server

    [root@k8s-master01 k8s-ha-install]# kubectl apply -f metrics-server/
    clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
    rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
    apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
    serviceaccount/metrics-server created
    deployment.extensions/metrics-server created
    service/metrics-server created
    clusterrole.rbac.authorization.k8s.io/system:metrics-server created
    clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created

查看pod

    [root@k8s-master01 k8s-ha-install]# kubectl get pods -n kube-system -o wide
    NAME   READY STATUSRESTARTS   AGE   IP  NODE
    .....
    metrics-server-55fcc5b88-ncmld 1/1   Running   0  3m172.168.3.2 k8s-node01

master节点上安装heapster，从v1.11.0开始，性能采集不再采用heapster采集pod性能数据，而是使用metrics-server，但是dashboard依然使用heapster呈现性能数据

    [root@k8s-master01 k8s-ha-install]# kubectl apply -f heapster/
    deployment.extensions/monitoring-grafana created
    service/monitoring-grafana created
    clusterrolebinding.rbac.authorization.k8s.io/heapster created
    clusterrole.rbac.authorization.k8s.io/heapster created
    serviceaccount/heapster created
    deployment.extensions/heapster created
    service/heapster created
    deployment.extensions/monitoring-influxdb created
    service/monitoring-influxdb created

查看pods

    [root@k8s-master01 k8s-ha-install]# kubectl get pods -n kube-system -o wide
    NAME   READY STATUSRESTARTS   AGE   IP  NODE
    .....
    heapster-5874d498f5-75r58  1/1   Running   0  25s   172.168.3.3 k8s-node01
    .....
    monitoring-grafana-9b6b75b49-4zm6d 1/1   Running   0  26s   172.168.2.2 k8s-master03
    monitoring-influxdb-655cd78874-lmz5l   1/1   Running   0  24s   172.168.0.2 k8s-master01
    .....

部署dashboard

    [root@k8s-master01 k8s-ha-install]# kubectl apply -f dashboard/
    secret/kubernetes-dashboard-certs created
    serviceaccount/kubernetes-dashboard created
    role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
    rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
    deployment.apps/kubernetes-dashboard created
    service/kubernetes-dashboard created
    serviceaccount/admin-user created
    clusterrolebinding.rbac.authorization.k8s.io/admin-user created

查看最终pods

    [root@k8s-master01 k8s-ha-install]# kubectl get pods -n kube-system -o wide
    NAMEREADY STATUSRESTARTS   AGE   IP  NODE
    calico-node-47bmt   2/2   Running   0  49m   192.168.20.30   k8s-node01
    calico-node-5r9ln   2/2   Running   1  43m   192.168.20.31   k8s-node02
    calico-node-kwz9t   2/2   Running   0  2h192.168.20.20   k8s-master01
    calico-node-rj8p8   2/2   Running   0  2h192.168.20.21   k8s-master02
    calico-node-xfsg5   2/2   Running   0  2h192.168.20.22   k8s-master03
    coredns-777d78ff6f-4rcsb1/1   Running   0  3h172.168.1.3 k8s-master02
    coredns-777d78ff6f-7xqzx1/1   Running   0  3h172.168.1.2 k8s-master02
    etcd-k8s-master01   1/1   Running   0  3h192.168.20.20   k8s-master01
    etcd-k8s-master02   1/1   Running   0  3h192.168.20.21   k8s-master02
    etcd-k8s-master03   1/1   Running   9  2h192.168.20.22   k8s-master03
    heapster-5874d498f5-75r58   1/1   Running   0  2m172.168.3.3 k8s-node01
    kube-apiserver-k8s-master01 1/1   Running   0  2h192.168.20.20   k8s-master01
    kube-apiserver-k8s-master02 1/1   Running   0  2h192.168.20.21   k8s-master02
    kube-apiserver-k8s-master03 1/1   Running   1  2h192.168.20.22   k8s-master03
    kube-controller-manager-k8s-master011/1   Running   0  2h192.168.20.20   k8s-master01
    kube-controller-manager-k8s-master021/1   Running   1  2h192.168.20.21   k8s-master02
    kube-controller-manager-k8s-master031/1   Running   0  2h192.168.20.22   k8s-master03
    kube-proxy-4cjhm1/1   Running   0  3h192.168.20.21   k8s-master02
    kube-proxy-ctjlh1/1   Running   0  43m   192.168.20.31   k8s-node02
    kube-proxy-lkvjk1/1   Running   2  3h192.168.20.22   k8s-master03
    kube-proxy-m7htq1/1   Running   0  3h192.168.20.20   k8s-master01
    kube-proxy-n9nzb1/1   Running   0  49m   192.168.20.30   k8s-node01
    kube-scheduler-k8s-master01 1/1   Running   2  3h192.168.20.20   k8s-master01
    kube-scheduler-k8s-master02 1/1   Running   0  3h192.168.20.21   k8s-master02
    kube-scheduler-k8s-master03 1/1   Running   2  2h192.168.20.22   k8s-master03
    kubernetes-dashboard-7954d796d8-mhpzq   1/1   Running   0  16s   172.168.0.3 k8s-master01
    metrics-server-55fcc5b88-ncmld  1/1   Running   0  9m172.168.3.2 k8s-node01
    monitoring-grafana-9b6b75b49-4zm6d  1/1   Running   0  2m172.168.2.2 k8s-master03
    monitoring-influxdb-655cd78874-lmz5l1/1   Running   0  2m172.168.0.2 k8s-master01

dashboard配置及访问：

访问：https://k8s-master-lb:30000
结合自身部署情况访问相应ip的端口进行服务访问(获取访问ip)。

    $kubectl describe secret/$(kubectl get secret -nkube-system |grep admin|awk '{print $1}') -nkube-system